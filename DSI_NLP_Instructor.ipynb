{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Natural Language Processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plug NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It'll bring up another window. From here, download: \n",
    "corpora -> movie_reviews\n",
    "\n",
    "corpora -> stopwords\n",
    "\n",
    "all packages -> punkt\n",
    "\n",
    "corpora -> wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import everything we need, explain as we use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break apart a sentence(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " 'sentence.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'break',\n",
       " 'everything',\n",
       " 'apart!']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"This is a test sentence. It will break everything apart!\"\n",
    "sentence.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it misses punctuation. Luckily, NLTK has a solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " 'sentence',\n",
       " '.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'break',\n",
       " 'everything',\n",
       " 'apart',\n",
       " '!']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"This is a test sentence. It will break everything apart!\"\n",
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have NLTK determine the part of speech (POS) for each token in our sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('test', 'NN'),\n",
       " ('sentence', 'NN'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('tag', 'VB'),\n",
       " ('everything', 'NN'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"This is a test sentence. It will tag everything!\"\n",
    "w = word_tokenize(sentence)\n",
    "nltk.pos_tag(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN means singular noun, JJ means adjective etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use NTLK to find the definition of words, synonyms, antonyms, etc.\n",
    "\n",
    "Some words have multiple definitions and multiple parts of speech depending on the usage. This can get very complicated very fast, so for this purpose, we're just going to assume that NLTK knows what its doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step we have are removing stopwords. Stop words are words that are used for grammaical purposes but carry little meaning (the, a, I, is, etc). \n",
    "\n",
    "So, let's remove them. Issue is, there are over 100 English words that are considered stopwords. So unless we want to create a list of stopwords and iterate every word over the list by hand everytime we write a program, we need a new solution.\n",
    "\n",
    "So, lets just let NLTK remove them for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our', 'symposium', 'will', 'also', 'include', 'two', 'rounds', 'of', 'workshops', 'with', 'several', 'choices', 'in', 'each', 'round-', 'so', 'you', 'can', 'brush', 'up', 'on', 'your', 'Python', ',', 'learn', 'about', 'data', 'visualization', ',', 'or', 'deepen', 'your', 'knowledge', 'of', 'machine', 'learning', '.']\n"
     ]
    }
   ],
   "source": [
    "para = \"Our symposium will also include two rounds of workshops with several choices in each round- so you can brush up on your Python, learn about data visualization, or deepen your knowledge of machine learning. \"\n",
    "words = word_tokenize(para)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our', 'symposium', 'also', 'include', 'two', 'rounds', 'workshops', 'several', 'choices', 'round-', 'brush', 'Python', ',', 'learn', 'data', 'visualization', ',', 'deepen', 'knowledge', 'machine', 'learning', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(para)\n",
    "useful_words = [word for word in words if word not in stopwords.words('english')]\n",
    "print(useful_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that \"The\" was not removed even though it is a stopword. Thats because NLTK's list is only in lowercase. So let's move our paragraph to lowercase first so it doesn't miss any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['symposium', 'also', 'include', 'two', 'rounds', 'workshops', 'several', 'choices', 'round-', 'brush', 'python', ',', 'learn', 'data', 'visualization', ',', 'deepen', 'knowledge', 'machine', 'learning', '.']\n"
     ]
    }
   ],
   "source": [
    "para = para.lower()\n",
    "words = word_tokenize(para)\n",
    "useful_words = [word for word in words if word not in stopwords.words('english')]\n",
    "print(useful_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new shorter list means that we don't need to process over as many words, saving us time and making things more efficient without getting rid of any meaning.\n",
    "\n",
    "Now, lets start with creating our tool.\n",
    "\n",
    "Machine learning works by learning from a set of data and then applying what its learned to a new set of data.\n",
    "\n",
    "So, the first thing we need is some data. This data can be tweets, reviews, books, anything really. We're going to be using movie reviews today.\n",
    "\n",
    "Lets explore the data a little bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "movie_reviews.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews.fileids()[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what the most common words are in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 77717),\n",
       " ('the', 76529),\n",
       " ('.', 65876),\n",
       " ('a', 38106),\n",
       " ('and', 35576),\n",
       " ('of', 34123),\n",
       " ('to', 31937),\n",
       " (\"'\", 30585),\n",
       " ('is', 25195),\n",
       " ('in', 21822),\n",
       " ('s', 18513),\n",
       " ('\"', 17612),\n",
       " ('it', 16107),\n",
       " ('that', 15924),\n",
       " ('-', 15595),\n",
       " (')', 11781),\n",
       " ('(', 11664),\n",
       " ('as', 11378),\n",
       " ('with', 10792),\n",
       " ('for', 9961)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = movie_reviews.words()\n",
    "freq_dist = nltk.FreqDist(all_words)\n",
    "freq_dist.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how a lot of these are stopwords. Its a good thing we know how to remove those!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have most of the tools we need. Let's get started.\n",
    "\n",
    "We're going to be using a Naive Bayes classifier but due to time constraints we're not going to get into how the classifier works and just work with it as a black box.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start looking at the reviews themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...], 'neg')\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "neg_reviews = []\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    words = movie_reviews.words(fileid)\n",
    "    neg_reviews.append((words, \"neg\"))\n",
    "print(neg_reviews[0])\n",
    "print(len(neg_reviews))\n",
    "\n",
    "\n",
    "pos_reviews = []\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    words = movie_reviews.words(fileid)\n",
    "    pos_reviews.append((words, \"pos\"))\n",
    "print(len(pos_reviews))\n",
    "    \n",
    "#print(len(pos_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 1000 positive reviews and 1000 negative reviews in the format we want. We still need to break these into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 200\n"
     ]
    }
   ],
   "source": [
    "train_set = neg_reviews[:300] + pos_reviews[:300]\n",
    "test_set =  neg_reviews[950:] + pos_reviews[950:]\n",
    "print(len(train_set),  len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets put this into our classifier through TextBlob's implimentation of Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "cl = NaiveBayesClassifier(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might take a second. But when it's done, we have our algorithm trained!\n",
    "\n",
    "In the meantime, let's go over what training and testing sets are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
